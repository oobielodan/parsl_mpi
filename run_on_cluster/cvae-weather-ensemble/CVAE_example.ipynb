{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Demo using CVAE on CORE2 data**\n",
    "This is based on an example from Keras.io by fchollet.  Please see last cell for original code and links.\n",
    "\n",
    "# Conda env Setup\n",
    "The following is the list steps needed to be taken in order to set up a correct conda environment for running this notebook.\n",
    "\n",
    "**Create Environment:**\n",
    "```bash\n",
    "source ~/pw/.miniconda3c/etc/profile.d/conda.sh\n",
    "conda create --name NAME python=3.9\n",
    "conda activate NAME\n",
    "```\n",
    "\n",
    "**Install Pacakages:** \n",
    "\n",
    "The following packages need to be installed on top of a typical base Conda env. Install the packages in the following order so the environment solves correctly:\n",
    "```bash\n",
    "conda install -y -c conda-forge tensorflow\n",
    "conda install -y -c conda-forge netCDF4          # For reading nc files\n",
    "conda install -y -c conda-forge cartopy          # For making maps\n",
    "conda install -y -c conda-forge matplotlib\n",
    "conda install -y -c conda-forge pandas\n",
    "conda install -y -c conda-forge scikit-learn\n",
    "conda install -y -c conda-forge cdo              # For converting grib to nc\n",
    "```\n",
    "**Connect Notebook to Environment:**\n",
    "```bash\n",
    "conda install -y ipykernel\n",
    "conda install -y requests\n",
    "conda install -y -c anaconda jinja2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import netCDF4\n",
    "import cartopy\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make needed directories\n",
    "!mkdir gefs_data\n",
    "!mkdir gefs_data/converted\n",
    "\n",
    "data_prefix = \"./gefs_data\"\n",
    "data_dir = \"/home/lobielodan/parsl_mpi/run_on_cluster/cvae-weather-ensemble/gefs_data/converted/\" # change to match your own directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Convert Data\n",
    "On my [first Google hit for GEFS](https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast), I clicked on [AWS Open Data Registry for GEFS](https://registry.opendata.aws/noaa-gefs-pds/) and selected [NOAA GEFS Re-forecast](https://registry.opendata.aws/noaa-gefs-reforecast/) which has no useage restrictions.  The [GEFS Re-forecast data documentation](https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf) is very clear and we're going to download two files, 57 MB each.  The date of the initialization of the re-forecast is in the file name in the format YYYYMMDDHH.  The c00, p01, p02, p03, p04 are the control and perturbation ensemble members (5 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data download\n",
    "def get_data(year, month, day, times, ensembles):\n",
    "    num_files = 0\n",
    "    \n",
    "    for time in times:\n",
    "        for ensemble in ensembles:\n",
    "            if f'pres_msl_{year}{month}{day}{time}_{ensemble}.grib2' not in os.listdir(data_prefix):\n",
    "                !wget -q -P {data_prefix} https://noaa-gefs-retrospective.s3.amazonaws.com/GEFSv12/reforecast/{year}/{year}{month}{day}{time}/{ensemble}/Days%3A1-10/pres_msl_{year}{month}{day}{time}_{ensemble}.grib2\n",
    "                num_files += 1\n",
    "            \n",
    "    return num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete all files\n",
    "def remove_data():\n",
    "    !find {data_prefix} -type f -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example for getting and converting files -> change the lists to fit your needs\n",
    "years = [\"2018\"]\n",
    "months = [\"01\"] \n",
    "days = [\"01\"] # forecasts are 10 days long, so (01, 10, 20) provides converage of whole year.\n",
    "times = [\"00\"] # every six hours (\"00\", \"06\", \"12\", \"18\", \"24\")\n",
    "ensembles = [\"c00\", \"p01\"]\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            num_files = get_data(year, month, day, times, ensembles)\n",
    "\n",
    "!csh batch_grib2nc.csh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example for loading data\n",
    "dataset = netCDF4.Dataset(data_dir + \"pres_msl_2018010100_c00.nc\")\n",
    "dataset2 = netCDF4.Dataset(data_dir + \"pres_msl_2018010100_p01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example for simple data access\n",
    "print(dataset) # look at data structure\n",
    "print(dataset.variables.keys())\n",
    "\n",
    "for var in dataset.variables:\n",
    "    print(dataset.variables[var])\n",
    "    # print(dataset.variables[var][:]) # prints actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example for plot\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = plt.axes(projection = cartopy.crs.LambertConformal())\n",
    "\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "ax.add_feature(cartopy.feature.LAKES, alpha = 0.5)\n",
    "ax.add_feature(cartopy.feature.STATES, edgecolor='grey')\n",
    "ax.set_extent([-120, -73, 23, 50])\n",
    "\n",
    "plt.contour(\n",
    "    dataset.variables['lon'][:],     # longitudes\n",
    "    dataset.variables['lat'][:],     # latitudes\n",
    "    dataset.variables['msl'][0,:,:], # actual data\n",
    "    transform = cartopy.crs.PlateCarree()) #, levels=np.arange(30000,110000,20000))\n",
    "\n",
    "plt.title('GEFSv12 SLP 2019 01 10 0000 UTC Cycle')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Design\n",
    "\n",
    "We need to get to a small latent space. Conv2D networks are good because they help reduce the number of connections in a network in a meaningful way.  I'm using terms as defined in [this definition of conv2D](https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148).\n",
    "\n",
    "**Definitions:**\n",
    "K -> kernel size;\n",
    "P -> padding;\n",
    "S -> stride;\n",
    "D -> Dilation;\n",
    "G -> Groups\n",
    "\n",
    "**Filter options:**\n",
    "Longitude is easy because it is large and even, so as long as you have an even stride, you get integer results when dividing.\n",
    "e.g. lon 9: stride 4, lat 7: stride 5\n",
    "\n",
    "- Latitude - whole numbers occurr for P = 2 & K = 3 or K = 11.\n",
    "- 11 grid points * 0.25 deg * 100 km/deg = 275 km filter window (a good scale for weather)\n",
    "- 9 grid points * 0.25 deg * 100 km/deg = 225 km\n",
    "- Longitude - whole numbers occur for P = 0 & K = 11 (nice match with Latitude), P = 1 & K = 3 or 13, P = 2 & K = 5.\n",
    "\n",
    "For a 5 x 7 filter with 3 stride (no overlap) and no padding:\n",
    "- lat: (721 - 4) / 3 = 239 possible steps (good whole number!)\n",
    "- lon: (1440 - 4) / 3 = 478.6666 possible steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try different filter sizes\n",
    "# Aim for large initial filter > 200km scale (about 10)\n",
    "# Aim for some, but minimal overlap in initial filter.\n",
    "\n",
    "# Aim for smallish second filter, but still try to reduce dimensionality\n",
    "# to make dense network tractable later. No overlap (but no good\n",
    "# reason why this is).\n",
    "\n",
    "# ----------------------Input: 721 x 1440--------------\n",
    "\n",
    "# For Lat = 721,\n",
    "# K = 11 -> K_radius = 5.0 -> S = 9 -> H_out = 79.0\n",
    "\n",
    "# For Lon = 1440,\n",
    "# K = 11 -> K_radius = 5.0 -> S = 10 -> H_out = 143.0\n",
    "\n",
    "# -----------------------Layer1: 79 x 143----------------\n",
    "\n",
    "# For Lat = 79,\n",
    "# K = 5 -> K_radius = 2.0 -> S = 5 -> H_out = 15.0\n",
    "\n",
    "# For Lon = 143,\n",
    "# K = 9 -> K_radius = 4.0 -> S = 9 -> H_out = 15.0\n",
    "\n",
    "H_in = 1440\n",
    "P = 0\n",
    "K_list = [3, 5, 7, 9, 11, 13, 15]                    # Kernel size\n",
    "S_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # Stride\n",
    "\n",
    "for K in K_list:\n",
    "    for S in S_list:\n",
    "        K_radius = np.floor(np.divide(K, 2))   # Half width of number of points around the central point\n",
    "        K_diameter = K - 1                     # Number of points around the central point, ASSUMES K = ODD\n",
    "        # S = K                                # S = K is stride necessary to have non-overlapping filters\n",
    "        print('K = ' + str(K) + ' -> K_radius = ' + str(K_radius) + ' -> S = ' + str(S) + ' -> H_out = ' + str((H_in + (2 * P) - K_diameter) / S))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampling Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the Encoder:\n",
    "GFS grids I have available here are at 0.25 degree resolution.  I'm doing this as a \"worst case\" scenario since there are also 0.5 and 1.0 degree grids with lower resolution but I can't find that data quickly and don't know what's available.\n",
    "\n",
    "These 0.25 degree grids are 721 x 1440.\n",
    "Each forecast file is 3 hourly for 10 days = 8 steps/forecast * 10 days = 80 \"frames\"\n",
    "This demo is only using two forecasts from the control ensemble\n",
    "(one launched Jan 01, 2019 and one launched Jan 02, 2019) -> this is only \n",
    "a small subset of the variability possible in the model.\n",
    "\n",
    "This particular data set spans 2000-2019 and there are 5 ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Memory usage before designing the neural network:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_encoder(latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=(721, 1440, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 11, activation = \"relu\", strides = [9, 10], padding = \"valid\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, [5,9], activation = \"relu\", strides = [5, 9], padding = \"valid\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    print(encoder.summary())\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Decoder:\n",
    "\n",
    "With the 11 x 11 and 5 x 5 filters, non-overlapping stride, applied here, we have a final \"image\" size of 14 x 27 and 64 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Why 7 x 7 x 64?\n",
    "# 7 x 7 \"image\" remaining after two conv2D operations x 64 filters/channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_decoder(latent_dim):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(15 * 15 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((15, 15, 64))(x)\n",
    "    # FIXME - there is something wrong here, but at least there is a pattern.\n",
    "    # Using output_padding as a fudge factor -> it may be that there is exactly\n",
    "    # one \"missing\" filter stamp/convolution because for both Conv2DTranspose\n",
    "    # operations, output_padding is set to maximum it could be in both dims\n",
    "    # (i.e. exactly one less than the stride of each filter).\n",
    "    x = layers.Conv2DTranspose(64, [5, 9], activation = \"relu\", strides = [5,9], padding = \"valid\", output_padding = [4, 8])(x)\n",
    "    x = layers.Conv2DTranspose(32, 11, activation = \"relu\", strides = [9,10], padding = \"valid\", output_padding = [8, 9])(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation = \"sigmoid\", padding = \"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name = \"decoder\")\n",
    "    \n",
    "    print(decoder.summary())\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the VAE as a `Model` with a Custom `train_step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "            n_features = 28 * 28\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "                )\n",
    "            ) / n_features\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "            total_loss = (reconstruction_loss + kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    # Needed to validate (validation loss) and to evaluate\n",
    "    def test_step(self, data):\n",
    "        if type(data) == tuple:\n",
    "            data, _ = data\n",
    "            \n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "        n_features = 28 * 28\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "            )\n",
    "        ) / n_features\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "        total_loss = (reconstruction_loss + kl_loss)\n",
    "        # grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only works if latent_dim = 2\n",
    "def plot_latent_space(vae, n = 30, figsize = 15, show = False, path = ''):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if path:\n",
    "        plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_images(images, rows, columns, show = False, path = ''):\n",
    "    fig = plt.figure(figsize = (10, 7))\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(image, cmap = 'binary')\n",
    "        plt.axis('off')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if path:\n",
    "        plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, X_valid):\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 40, restore_best_weights = True) # stops training early if the validation loss does not improve\n",
    "\n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): # if the model has already been trained at least once, load that model\n",
    "        vae.load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "\n",
    "    history = vae.fit(\n",
    "        X_train, epochs = 50, batch_size = 40,\n",
    "        callbacks = [early_stopping_cb],\n",
    "        validation_data = (X_valid,)\n",
    "    )\n",
    "\n",
    "    vae.save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "\n",
    "    hist_pd = pd.DataFrame(history.history)\n",
    "    hist_pd.to_csv(os.path.join(model_dir, 'history.csv'), index = False)\n",
    "\n",
    "    test_loss = vae.evaluate(X_test)\n",
    "    test_loss = dict(zip([\"loss\", \"reconstruction_loss\", \"kl_loss\"], test_loss))\n",
    "\n",
    "    print('Test loss:', test_loss)\n",
    "\n",
    "    with open(os.path.join(model_dir, 'test_loss.json'), 'w') as json_file:\n",
    "        json.dump(test_loss, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Memory usage before computation:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "train = True\n",
    "model_dir = './model_dir'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "# build encoder\n",
    "encoder = build_encoder(latent_dim)\n",
    "print(\"Memory usage after building encoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build decoder\n",
    "decoder = build_decoder(latent_dim)\n",
    "print(\"Memory usage after building decoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build VAE (variational autoencoder)\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer = 'rmsprop') \n",
    "# vae.compile(optimizer = keras.optimizers.Adam())\n",
    "print(\"Memory usage after building VAE:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Training Data:\n",
    "The standard way of manipulating arrays in Conv2D layers in TF is to use arrays in the shape:\n",
    "`batch_size,  height, width, channels = data.shape`\n",
    "In our case, the the `batch_size` is the number of image frames (i.e. separate samples or rows in a `.csv` file), the `height` and `width` define the size of the image frame in number of pixels, and the `channels` are the number of layers in the frames.  Typically, channels are color layers (e.g. RGB or CMYK) but in our case, we could use different metereological variables.  However, for this first experiment, **we only need one channel** because we're only going to use mean sea level pressure (msl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used MNIST data preproc as template for this definition\n",
    "def load_data(): \n",
    "    files = os.listdir(data_dir)\n",
    "    files = [f for f in files if '.nc' in f]\n",
    "    \n",
    "    all_data = np.expand_dims(\n",
    "        np.concatenate(\n",
    "            [netCDF4.Dataset(data_dir + converted_file)['msl'][:] for converted_file in files]\n",
    "        ),\n",
    "        -1\n",
    "    ).astype(\"float32\") / 110000\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_train(num_files):\n",
    "    slp = load_data() # load data\n",
    "    print(\"shape:\", np.shape(slp)) # verify data shape\n",
    "    \n",
    "    # split the data - y values are throw away\n",
    "    X_train, X_test, y_train, y_test = train_test_split(slp[0:(num_files * 80 - 1), :, :, :], np.arange(num_files * 80 - 1), test_size = 0.2, random_state = 1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    train_model(X_train, X_test, X_valid)\n",
    "    remove_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run training\n",
    "\n",
    "count = 0\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "months = [\"01\"]\n",
    "days = [\"01\", \"02\"]\n",
    "times = [\"00\"]\n",
    "ensembles = [\"c00\", \"p01\", \"p02\", \"p03\", \"p04\"]\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            count += 1\n",
    "            print(\"train\", count)\n",
    "\n",
    "            num_files = get_data(year, month, day, times, ensembles)\n",
    "            !csh batch_grib2nc.csh\n",
    "            \n",
    "            run_train(num_files)\n",
    "    \n",
    "print(\"Memory usage after training:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if latent_dim == 2:\n",
    "#     plot_latent_space(vae, path = os.path.join(model_dir, 'latent_space.png'))\n",
    "\n",
    "# # Generating new images\n",
    "# codings = tf.random.normal(shape = [12, latent_dim])\n",
    "# images = vae.decoder(codings).numpy()\n",
    "# plot_images(images, 3, 4, path = os.path.join(model_dir, 'generated.png'))\n",
    "\n",
    "# # Semantic interpolation\n",
    "# codings_grid = tf.reshape(codings, [1, 3, 4, latent_dim])\n",
    "# larger_grid = tf.image.resize(codings_grid, size = [5, 7])\n",
    "# interpolated_codings = tf.reshape(larger_grid, [-1, latent_dim])\n",
    "# images = vae.decoder(interpolated_codings).numpy()\n",
    "# plot_images(images, 5, 7, path = os.path.join(model_dir, 'interpolated.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display a grid of sampled digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_latent_space(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display how the latent space clusters different digit classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Original code\n",
    "CVAE example from [Keras.io](https://keras.io/examples/generative/vae/), [github repo](https://github.com/keras-team/keras-io/blob/master/examples/generative/vae.py).\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Title: Variational AutoEncoder\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2020/05/03\n",
    "Last modified: 2020/05/03\n",
    "Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Create a sampling layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the encoder\n",
    "\"\"\"\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Build the decoder\n",
    "\"\"\"\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Define the VAE as a `Model` with a custom `train_step`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Train the VAE\n",
    "\"\"\"\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128)\n",
    "\n",
    "\"\"\"\n",
    "## Display a grid of sampled digits\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=30, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(vae)\n",
    "\n",
    "\"\"\"\n",
    "## Display how the latent space clusters different digit classes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvae_env]",
   "language": "python",
   "name": "conda-env-cvae_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
