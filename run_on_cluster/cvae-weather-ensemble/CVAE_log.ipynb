{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6226ca5-0869-4973-8369-3469cbdff43c",
   "metadata": {
    "papermill": {
     "duration": 0.003516,
     "end_time": "2024-07-08T15:30:53.296453",
     "exception": false,
     "start_time": "2024-07-08T15:30:53.292937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CVAE_training_log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094dca89-dffa-4a56-8263-371fb372a2cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:53.304323Z",
     "iopub.status.busy": "2024-07-08T15:30:53.304025Z",
     "iopub.status.idle": "2024-07-08T15:30:56.681380Z",
     "shell.execute_reply": "2024-07-08T15:30:56.680654Z"
    },
    "papermill": {
     "duration": 3.383142,
     "end_time": "2024-07-08T15:30:56.683125",
     "exception": false,
     "start_time": "2024-07-08T15:30:53.299983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 15:30:54.378755: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.16.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "import papermill as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import netCDF4\n",
    "import cartopy\n",
    "import CVAE_helpers\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f7565b-a428-4a60-b085-42b5d1820c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.691370Z",
     "iopub.status.busy": "2024-07-08T15:30:56.690917Z",
     "iopub.status.idle": "2024-07-08T15:30:56.694597Z",
     "shell.execute_reply": "2024-07-08T15:30:56.693944Z"
    },
    "papermill": {
     "duration": 0.009681,
     "end_time": "2024-07-08T15:30:56.696424",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.686743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from CVAE_helpers import build_encoder # build encoder\n",
    "# from CVAE_helpers import build_decoder # build decoder\n",
    "# from CVAE_helpers import VAE           # build model\n",
    "# from CVAE_helpers import train_model   # train model\n",
    "\n",
    "# from CVAE_helpers import load_data     # load_data\n",
    "# from CVAE_helpers import run_train     # run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8b533-cdaa-4efd-938b-ef1d5a84338e",
   "metadata": {
    "papermill": {
     "duration": 0.00267,
     "end_time": "2024-07-08T15:30:56.701698",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.699028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download and Convert Data\n",
    "On my [first Google hit for GEFS](https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast), I clicked on [AWS Open Data Registry for GEFS](https://registry.opendata.aws/noaa-gefs-pds/) and selected [NOAA GEFS Re-forecast](https://registry.opendata.aws/noaa-gefs-reforecast/) which has no useage restrictions.  The [GEFS Re-forecast data documentation](https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf) is very clear and we're going to download two files, 57 MB each.  The date of the initialization of the re-forecast is in the file name in the format YYYYMMDDHH.  The c00, p01, p02, p03, p04 are the control and perturbation ensemble members (5 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b08e8b0-f9fa-40b9-852b-8650e681cff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.708078Z",
     "iopub.status.busy": "2024-07-08T15:30:56.707847Z",
     "iopub.status.idle": "2024-07-08T15:30:56.713905Z",
     "shell.execute_reply": "2024-07-08T15:30:56.713238Z"
    },
    "papermill": {
     "duration": 0.011576,
     "end_time": "2024-07-08T15:30:56.715965",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.704389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_prefix = \"./gefs_data\"\n",
    "\n",
    "# data download\n",
    "def get_data(year, month, day, ensembles):\n",
    "    num_files = 0\n",
    "    \n",
    "    for ensemble in ensembles:\n",
    "        if f'pres_msl_{year}{month}{day}00_{ensemble}.grib2' not in os.listdir(data_prefix):\n",
    "            !wget -q -P {data_prefix} https://noaa-gefs-retrospective.s3.amazonaws.com/GEFSv12/reforecast/{year}/{year}{month}{day}00/{ensemble}/Days%3A1-10/pres_msl_{year}{month}{day}00_{ensemble}.grib2\n",
    "        \n",
    "        if f'pres_msl_{year}{month}{day}00_{ensemble}.grib2' in os.listdir(data_prefix):\n",
    "            num_files += 1\n",
    "            \n",
    "    return num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff4403c-9c45-4129-902b-93ebfe9749e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.722206Z",
     "iopub.status.busy": "2024-07-08T15:30:56.721982Z",
     "iopub.status.idle": "2024-07-08T15:30:56.725576Z",
     "shell.execute_reply": "2024-07-08T15:30:56.724968Z"
    },
    "papermill": {
     "duration": 0.008756,
     "end_time": "2024-07-08T15:30:56.727304",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.718548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete all files\n",
    "def remove_data():\n",
    "    !find {data_prefix} -type f -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ebb2a-bc7b-4f52-98d3-ec85c51c7a5f",
   "metadata": {
    "papermill": {
     "duration": 0.002694,
     "end_time": "2024-07-08T15:30:56.732619",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.729925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Neural Network Design\n",
    "\n",
    "We need to get to a small latent space. Conv2D networks are good because they help reduce the number of connections in a network in a meaningful way.  I'm using terms as defined in [this definition of conv2D](https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148).\n",
    "\n",
    "**Definitions:**\n",
    "K -> kernel size;\n",
    "P -> padding;\n",
    "S -> stride;\n",
    "D -> Dilation;\n",
    "G -> Groups\n",
    "\n",
    "**Filter options:**\n",
    "Longitude is easy because it is large and even, so as long as you have an even stride, you get integer results when dividing.\n",
    "e.g. lon 9: stride 4, lat 7: stride 5\n",
    "\n",
    "- Latitude - whole numbers occurr for P = 2 & K = 3 or K = 11.\n",
    "- 11 grid points * 0.25 deg * 100 km/deg = 275 km filter window (a good scale for weather)\n",
    "- 9 grid points * 0.25 deg * 100 km/deg = 225 km\n",
    "- Longitude - whole numbers occur for P = 0 & K = 11 (nice match with Latitude), P = 1 & K = 3 or 13, P = 2 & K = 5.\n",
    "\n",
    "For a 5 x 7 filter with 3 stride (no overlap) and no padding:\n",
    "- lat: (721 - 4) / 3 = 239 possible steps (good whole number!)\n",
    "- lon: (1440 - 4) / 3 = 478.6666 possible steps\n",
    "\n",
    "## Load and Preprocess Training Data:\n",
    "The standard way of manipulating arrays in Conv2D layers in TF is to use arrays in the shape:\n",
    "`batch_size,  height, width, channels = data.shape`\n",
    "In our case, the the `batch_size` is the number of image frames (i.e. separate samples or rows in a `.csv` file), the `height` and `width` define the size of the image frame in number of pixels, and the `channels` are the number of layers in the frames.  Typically, channels are color layers (e.g. RGB or CMYK) but in our case, we could use different metereological variables.  However, for this first experiment, **we only need one channel** because we're only going to use mean sea level pressure (msl).\n",
    "\n",
    "## Build the Encoder:\n",
    "GFS grids I have available here are at 0.25 degree resolution.  I'm doing this as a \"worst case\" scenario since there are also 0.5 and 1.0 degree grids with lower resolution but I can't find that data quickly and don't know what's available.\n",
    "\n",
    "These 0.25 degree grids are 721 x 1440.\n",
    "Each forecast file is 3 hourly for 10 days = 8 steps/forecast * 10 days = 80 \"frames\"\n",
    "This demo is only using two forecasts from the control ensemble\n",
    "(one launched Jan 01, 2019 and one launched Jan 02, 2019) -> this is only \n",
    "a small subset of the variability possible in the model.\n",
    "\n",
    "This particular data set spans 2000-2019 and there are 5 ensemble members.\n",
    "\n",
    "## Build the Decoder:\n",
    "With the 11 x 11 and 5 x 5 filters, non-overlapping stride, applied here, we have a final \"image\" size of 14 x 27 and 64 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0144303-f1c9-4220-8178-3988d707688c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.739282Z",
     "iopub.status.busy": "2024-07-08T15:30:56.739057Z",
     "iopub.status.idle": "2024-07-08T15:30:56.758020Z",
     "shell.execute_reply": "2024-07-08T15:30:56.757350Z"
    },
    "papermill": {
     "duration": 0.024605,
     "end_time": "2024-07-08T15:30:56.759819",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.735214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=(721, 1440, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 11, activation = \"relu\", strides = [9, 10], padding = \"valid\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, [5,9], activation = \"relu\", strides = [5, 9], padding = \"valid\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    print(encoder.summary())\n",
    "    return encoder\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(15 * 15 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((15, 15, 64))(x)\n",
    "    # FIXME - there is something wrong here, but at least there is a pattern.\n",
    "    # Using output_padding as a fudge factor -> it may be that there is exactly\n",
    "    # one \"missing\" filter stamp/convolution because for both Conv2DTranspose\n",
    "    # operations, output_padding is set to maximum it could be in both dims\n",
    "    # (i.e. exactly one less than the stride of each filter).\n",
    "    x = layers.Conv2DTranspose(64, [5, 9], activation = \"relu\", strides = [5,9], padding = \"valid\", output_padding = [4, 8])(x)\n",
    "    x = layers.Conv2DTranspose(32, 11, activation = \"relu\", strides = [9,10], padding = \"valid\", output_padding = [8, 9])(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation = \"sigmoid\", padding = \"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name = \"decoder\")\n",
    "    \n",
    "    print(decoder.summary())\n",
    "    return decoder\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "            n_features = 28 * 28\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "                )\n",
    "            ) / n_features\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "            total_loss = (reconstruction_loss + kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    # Needed to validate (validation loss) and to evaluate\n",
    "    def test_step(self, data):\n",
    "        if type(data) == tuple:\n",
    "            data, _ = data\n",
    "            \n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "        n_features = 28 * 28\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "            )\n",
    "        ) / n_features\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "        total_loss = (reconstruction_loss + kl_loss)\n",
    "        # grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61f638f-42a9-4d44-8f31-c6c42017a6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.766517Z",
     "iopub.status.busy": "2024-07-08T15:30:56.766252Z",
     "iopub.status.idle": "2024-07-08T15:30:56.775884Z",
     "shell.execute_reply": "2024-07-08T15:30:56.775071Z"
    },
    "papermill": {
     "duration": 0.015147,
     "end_time": "2024-07-08T15:30:56.777716",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.762569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, X_valid, date):\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True) # stops training early if the validation loss does not improve\n",
    "\n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): # if the model has already been trained at least once, load that model\n",
    "        vae.load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "        # reloaded = tf.saved_model.load(model_dir)\n",
    "\n",
    "    history = vae.fit(\n",
    "        X_train, epochs = 50, batch_size = 40,\n",
    "        callbacks = [early_stopping_cb],\n",
    "        validation_data = (X_valid,)\n",
    "    )\n",
    "\n",
    "    vae.save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "    # tf.saved_model.save(vae, model_dir)\n",
    "\n",
    "    hist_pd = pd.DataFrame(history.history)\n",
    "    hist_pd.to_csv(os.path.join(model_dir, f'history_{date}.csv'), index = False)\n",
    "\n",
    "    test_loss = vae.evaluate(X_test)\n",
    "    test_loss = dict(zip([\"loss\", \"reconstruction_loss\", \"kl_loss\"], test_loss))\n",
    "\n",
    "    print('Test loss:', test_loss)\n",
    "\n",
    "    with open(os.path.join(model_dir, f'test_loss_{date}.json'), 'w') as json_file:\n",
    "        json.dump(test_loss, json_file, indent = 4)\n",
    "\n",
    "data_dir = data_prefix + '/' + 'converted/'\n",
    "\n",
    "# used MNIST data preproc as template for this definition\n",
    "def load_data(): \n",
    "    files = os.listdir(data_dir)\n",
    "    files = [f for f in files if '.nc' in f]\n",
    "    \n",
    "    all_data = np.expand_dims(\n",
    "        np.concatenate(\n",
    "            [netCDF4.Dataset(data_dir + converted_file)['msl'][:] for converted_file in files]\n",
    "        ),\n",
    "        -1\n",
    "    ).astype(\"float32\") / 110000\n",
    "    return all_data\n",
    "\n",
    "def run_train(num_files, date):\n",
    "    slp = load_data() # load data\n",
    "    print(\"shape:\", np.shape(slp)) # verify data shape\n",
    "    \n",
    "    # split the data - y values are throw away\n",
    "    X_train, X_test, y_train, y_test = train_test_split(slp[0:(num_files * 80 - 1), :, :, :], np.arange(0, num_files * 80 - 1), test_size = 0.2, random_state = 1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    train_model(X_train, X_test, X_valid, date)\n",
    "    remove_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defdd05-1774-4e56-bd64-2f760c004842",
   "metadata": {
    "papermill": {
     "duration": 0.002734,
     "end_time": "2024-07-08T15:30:56.783111",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.780377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd6a16c-2ff9-42fe-bdcd-b8a970f88b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:56.789557Z",
     "iopub.status.busy": "2024-07-08T15:30:56.789295Z",
     "iopub.status.idle": "2024-07-08T15:30:57.483869Z",
     "shell.execute_reply": "2024-07-08T15:30:57.482944Z"
    },
    "papermill": {
     "duration": 0.699885,
     "end_time": "2024-07-08T15:30:57.485676",
     "exception": false,
     "start_time": "2024-07-08T15:30:56.785791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">721</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1440</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">143</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,904</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,224</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14400</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">230,416</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m721\u001b[0m, \u001b[38;5;34m1440\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m143\u001b[0m,   │      \u001b[38;5;34m3,904\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m,    │     \u001b[38;5;34m92,224\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14400\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │    \u001b[38;5;34m230,416\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling (\u001b[38;5;33mSampling\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,612</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m326,612\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,612</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m326,612\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Memory usage after building encoder: {'current': 1311232, 'peak': 3153408}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14400</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">143</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">184,384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">721</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1440</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">247,840</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">721</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1440</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14400\u001b[0m)          │        \u001b[38;5;34m43,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m143\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │       \u001b[38;5;34m184,384\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m721\u001b[0m, \u001b[38;5;34m1440\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │       \u001b[38;5;34m247,840\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m721\u001b[0m, \u001b[38;5;34m1440\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │           \u001b[38;5;34m289\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">475,713</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m475,713\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">475,713</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m475,713\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Memory usage after building decoder: {'current': 3216640, 'peak': 6049536}\n",
      "Memory usage after building VAE: {'current': 3219200, 'peak': 6049536}\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "train = True\n",
    "model_dir = './model_dir'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "# build encoder\n",
    "encoder = build_encoder(latent_dim)\n",
    "print(\"Memory usage after building encoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build decoder\n",
    "decoder = build_decoder(latent_dim)\n",
    "print(\"Memory usage after building decoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build VAE (variational autoencoder)\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer = 'rmsprop') \n",
    "# vae.compile(optimizer = keras.optimizers.Adam())\n",
    "print(\"Memory usage after building VAE:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c79096d-dee9-4ffc-9279-442e9232f20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:57.497039Z",
     "iopub.status.busy": "2024-07-08T15:30:57.496773Z",
     "iopub.status.idle": "2024-07-08T15:30:57.500644Z",
     "shell.execute_reply": "2024-07-08T15:30:57.499879Z"
    },
    "papermill": {
     "duration": 0.011024,
     "end_time": "2024-07-08T15:30:57.502386",
     "exception": false,
     "start_time": "2024-07-08T15:30:57.491362",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameter cell for pm\n",
    "year = \"2018\"\n",
    "month = \"01\"\n",
    "day = \"01\"\n",
    "ensembles = [\"c00\", \"p01\", \"p02\", \"p03\", \"p04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc072202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:57.511779Z",
     "iopub.status.busy": "2024-07-08T15:30:57.511533Z",
     "iopub.status.idle": "2024-07-08T15:30:57.514976Z",
     "shell.execute_reply": "2024-07-08T15:30:57.514355Z"
    },
    "papermill": {
     "duration": 0.010358,
     "end_time": "2024-07-08T15:30:57.516836",
     "exception": false,
     "start_time": "2024-07-08T15:30:57.506478",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "year = \"2018\"\n",
    "month = \"08\"\n",
    "day = \"30\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b952473d-6ec6-4581-8b49-d6d472b4e0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:30:57.526155Z",
     "iopub.status.busy": "2024-07-08T15:30:57.525917Z",
     "iopub.status.idle": "2024-07-08T15:37:59.562023Z",
     "shell.execute_reply": "2024-07-08T15:37:59.560850Z"
    },
    "papermill": {
     "duration": 422.043132,
     "end_time": "2024-07-08T15:37:59.564025",
     "exception": false,
     "start_time": "2024-07-08T15:30:57.520893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./gefs_data/pres_msl_2018083000_c00.grib2\n",
      "cdo    copy:   1\u001b[32mcdo    copy: \u001b[0mProcessed 83059200 values from 1 variable over 80 timesteps [1.69s 84MB]\n",
      "Working on ./gefs_data/pres_msl_2018083000_p01.grib2\n",
      "cdo    copy:   1\u001b[32mcdo    copy: \u001b[0mProcessed 83059200 values from 1 variable over 80 timesteps [1.72s 84MB]\n",
      "Working on ./gefs_data/pres_msl_2018083000_p02.grib2\n",
      "cdo    copy:   1\u001b[32mcdo    copy: \u001b[0mProcessed 83059200 values from 1 variable over 80 timesteps [1.70s 85MB]\n",
      "Working on ./gefs_data/pres_msl_2018083000_p03.grib2\n",
      "cdo    copy:   1\u001b[32mcdo    copy: \u001b[0mProcessed 83059200 values from 1 variable over 80 timesteps [1.70s 84MB]\n",
      "Working on ./gefs_data/pres_msl_2018083000_p04.grib2\n",
      "cdo    copy:   1\u001b[32mcdo    copy: \u001b[0mProcessed 83059200 values from 1 variable over 80 timesteps [1.70s 84MB]\n",
      "shape: (400, 721, 1440, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lobielodan/pw/.miniconda3c/envs/cvae_env/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 26s/step - kl_loss: 0.0016 - loss: 378.1416 - reconstruction_loss: 378.1401 - val_kl_loss: 0.0016 - val_loss: 377.5125 - val_reconstruction_loss: 377.5110\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 487ms/step - kl_loss: 0.0016 - loss: 377.8886 - reconstruction_loss: 377.8871 - val_kl_loss: 0.0016 - val_loss: 377.5059 - val_reconstruction_loss: 377.5043\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 486ms/step - kl_loss: 0.0016 - loss: 377.7009 - reconstruction_loss: 377.6993 - val_kl_loss: 0.0016 - val_loss: 377.6522 - val_reconstruction_loss: 377.6506\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 485ms/step - kl_loss: 0.0016 - loss: 377.8210 - reconstruction_loss: 377.8194 - val_kl_loss: 0.0016 - val_loss: 377.5172 - val_reconstruction_loss: 377.5155\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 487ms/step - kl_loss: 0.0016 - loss: 377.7442 - reconstruction_loss: 377.7426 - val_kl_loss: 0.0016 - val_loss: 377.5333 - val_reconstruction_loss: 377.5317\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 488ms/step - kl_loss: 0.0016 - loss: 377.8270 - reconstruction_loss: 377.8254 - val_kl_loss: 0.0016 - val_loss: 377.5624 - val_reconstruction_loss: 377.5608\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 488ms/step - kl_loss: 0.0016 - loss: 377.6180 - reconstruction_loss: 377.6164 - val_kl_loss: 0.0016 - val_loss: 377.5199 - val_reconstruction_loss: 377.5183\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 17s/step - kl_loss: 0.0016 - loss: 377.1447 - reconstruction_loss: 377.1432\n",
      "Test loss: {'loss': 377.2026062011719, 'reconstruction_loss': 0.0015868665650486946, 'kl_loss': 377.2041931152344}\n",
      "Memory usage after training: {'current': 6921728, 'peak': 18028204032}\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "num_files = get_data(year, month, day, ensembles)\n",
    "!csh batch_grib2nc.csh\n",
    "\n",
    "date = year + month + day\n",
    "run_train(num_files, date)\n",
    "    \n",
    "print(\"Memory usage after training:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvae_env]",
   "language": "python",
   "name": "conda-env-cvae_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 430.20903,
   "end_time": "2024-07-08T15:38:02.655823",
   "environment_variables": {},
   "exception": null,
   "input_path": "CVAE_training.ipynb",
   "output_path": "CVAE_log.ipynb",
   "parameters": {
    "day": "30",
    "month": "08",
    "year": "2018"
   },
   "start_time": "2024-07-08T15:30:52.446793",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
